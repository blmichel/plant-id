{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..')) # modify to point to plant_id\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ed72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plant_id\n",
    "from plant_id import callbacks as cb\n",
    "from plant_id import lit_models\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_info, rank_zero_only\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3877b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.util import DATA_CLASS_MODULE, import_class, MODEL_CLASS_MODULE, setup_data_and_model_from_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cbcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plant_id.metadata.inat as metadata\n",
    "import yaml\n",
    "config_file = \"training_config.yml\"\n",
    "with open(config_file, \"rb\") as file:\n",
    "        config = yaml.load(file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate run name\n",
    "WANDB_RUN_NAME = f\"{config['PRETRAINED_STEM']} wd={config['WEIGHT_DECAY']}\"\n",
    "if config['REDUCE_LR_ON_PLATEAU']:\n",
    "    WANDB_RUN_NAME += f\" ROP={config['ROP_COEFF']}\"\n",
    "if config['LAYERWISE_LR_DECAY']:\n",
    "    WANDB_RUN_NAME += f\" LLRD={config['LLRD_COEFF']}\"\n",
    "if not config['HEAD_DWSCONV']:\n",
    "    WANDB_RUN_NAME += ' head_dws=False'\n",
    "if config['TRAIN_SET'] == 'full':\n",
    "    WANDB_RUN_NAME += ' ds=full'\n",
    "\n",
    "print(WANDB_RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process args\n",
    "if config['AUTO_LR_FIND']:\n",
    "    lr = 'autotune'\n",
    "else:\n",
    "    lr = config['LR']\n",
    "\n",
    "if config['AUTO_SCALE_BATCH_SIZE']==None:\n",
    "    auto_scale_batch_size = 'False'\n",
    "else:\n",
    "    auto_scale_batch_size = config['AUTO_SCALE_BATCH_SIZE']\n",
    "\n",
    "if config['LOADED_MODEL']==None:\n",
    "    loaded_model = 'None'\n",
    "else:\n",
    "    loaded_model = config['LOADED_MODEL']\n",
    "\n",
    "FC_DIM = metadata.NUM_PLANT_CLASSES\n",
    "## wandb metadata\n",
    "training_config = dict (\n",
    "        dataset_id = \"iNat-2021\",\n",
    "        infra = \"BLM MBA\",\n",
    "        pretrained_stem = config['PRETRAINED_STEM'],\n",
    "        resolution = config['RESOLUTION'],\n",
    "        learning_rate = lr,\n",
    "        batch_size = config['BATCH_SIZE'], # need better logging: if autoscaled, this isn't the actual bs\n",
    "        auto_scale_batch_size = auto_scale_batch_size,\n",
    "        weight_decay = config['WEIGHT_DECAY'],\n",
    "        fc_dropout = config['FC_DROPOUT'],\n",
    "        fc_dim = FC_DIM,\n",
    "        precision = config['PRECISION'],\n",
    "        limit_train_batches = config['LIMIT_TRAIN_BATCHES'],\n",
    "        dataset_type = config['TRAIN_SET'],\n",
    "        loaded_model = loaded_model,\n",
    "        reduce_lr_on_plateau = config['REDUCE_LR_ON_PLATEAU'],\n",
    "        use_swa = config['STOCHASTIC_WEIGHT_AVERAGING'],\n",
    "        early_stopping = config['EARLY_STOPPING'],\n",
    "        loss = config['LOSS'],\n",
    "        layerwise_lr_decay = config['LAYERWISE_LR_DECAY'],\n",
    "        layerwise_lr_decay_coeff = config['LLRD_COEFF'],\n",
    "        head_dwsconv = config['HEAD_DWSCONV'],\n",
    "        rop_coeff = config['ROP_COEFF'],\n",
    "        rop_threshold = config['ROP_THRESHOLD'],\n",
    "        rop_threshold_mode = config['ROP_THRESHOLD_MODE'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_stem = training_config['pretrained_stem']\n",
    "print(\"Pretrained base model: \", pretrained_stem)\n",
    "\n",
    "## different models have different names for stem and blocks\n",
    "if 'lambda' in pretrained_stem:\n",
    "    mode = 'lambda'\n",
    "elif 'resnet' in pretrained_stem:\n",
    "    mode = 'resnet'\n",
    "elif 'efficientnet' in pretrained_stem:\n",
    "    mode = 'efficientnet'\n",
    "elif 'convnext' in pretrained_stem:\n",
    "    mode = 'convnext'\n",
    "elif 'resnext' in pretrained_stem:\n",
    "    mode = 'resnext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "import plant_id.models as models\n",
    "import plant_id.data as data\n",
    "#import plant_id.util\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "!echo $PYTORCH_ENABLE_MPS_FALLBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b26b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set log dir and format filename\n",
    "#log_dir = Path(\"training\") / \"logs\"\n",
    "log_dir = 'logs'\n",
    "logger = pl.loggers.TensorBoardLogger(log_dir)\n",
    "experiment_dir = logger.log_dir\n",
    "\n",
    "goldstar_metric = \"validation/acc\"\n",
    "filename_format = \"epoch={epoch:04d}-validation.loss={validation/loss:.3f}\"\n",
    "if goldstar_metric == \"validation/acc\":\n",
    "    filename_format += \"-validation.acc={validation/acc:.3f}\"\n",
    "\n",
    "## callbacks\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=2,\n",
    "    filename=filename_format,\n",
    "    monitor=goldstar_metric,\n",
    "    mode=\"max\",\n",
    "    auto_insert_metric_name=False,\n",
    "    dirpath=experiment_dir,\n",
    ")\n",
    "lr_callback = pl.callbacks.LearningRateMonitor()\n",
    "callbacks = [checkpoint_callback, lr_callback, ]\n",
    "if config['EARLY_STOPPING']:\n",
    "    callbacks.append(pl.callbacks.EarlyStopping(\n",
    "    monitor=\"validation/acc\",\n",
    "    mode=\"max\",\n",
    "    patience=1,\n",
    "    ))\n",
    "if config['STOCHASTIC_WEIGHT_AVERAGING']:\n",
    "    callbacks.append(pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-4,\n",
    "                                                            swa_epoch_start=0.5,\n",
    "                                                            annealing_epochs=5,\n",
    "    ))\n",
    "\n",
    "## set up lit model and datamodule\n",
    "DATA_CONFIG = {\"input_dims\" : (3, training_config['resolution'], training_config['resolution'])}\n",
    "MODEL_CONFIG = {\"pretrained_stem\" : pretrained_stem, \"fc_dim\": FC_DIM,\n",
    "                \"fc_dropout\" : training_config['fc_dropout'], \"mode\": mode}    \n",
    "\n",
    "model = models.FinetuningCNN(data_config=DATA_CONFIG, model_config=MODEL_CONFIG)\n",
    "lit_model = lit_models.LitFinetuningCNN(model)\n",
    "datamodule = data.iNatDataModule()\n",
    "\n",
    "if config['LOADED_MODEL'] != 'None':\n",
    "    loaded_lit_model = torch.load(config['LOADED_MODEL'])\n",
    "    #TODO: replace with load_state_dict, this currently doesn't work\n",
    "    lit_model.state_dict = loaded_lit_model.state_dict # need this step for wandb to log\n",
    "    del(loaded_lit_model)\n",
    "    \n",
    "# WATCH OUT HARDCODED HERE\n",
    "use_wandb = False\n",
    "\n",
    "## optionally, watch model with wandb    \n",
    "if use_wandb: #args.wandb\n",
    "    wandb.init(\n",
    "        project=config['WANDB_PROJECT_NAME'],\n",
    "        notes=\"testing wandb integration\",\n",
    "        name=WANDB_RUN_NAME,\n",
    "        tags=[\"test\"],\n",
    "        config=training_config,\n",
    "    )\n",
    "    logger = pl.loggers.WandbLogger(log_model=\"all\", save_dir=str(log_dir), job_type=\"train\")\n",
    "    logger.watch(lit_model)\n",
    "    experiment_dir = logger.experiment.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "## training code\n",
    "trainer = pl.Trainer(max_epochs=config['NUM_EPOCHS'],\n",
    "                     devices=-1, # can only address one GPU on mps?\n",
    "                     accelerator='mps', #heyo\n",
    "                     callbacks=callbacks, logger=logger,\n",
    "                     auto_scale_batch_size=None,\n",
    "                     auto_lr_find=False,\n",
    "#                     precision=config['PRECISION'],\n",
    "                     limit_train_batches=config['LIMIT_TRAIN_BATCHES'],\n",
    "                     )\n",
    "trainer.tune(lit_model, datamodule=datamodule)\n",
    "trainer.fit(lit_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fdad5",
   "metadata": {},
   "source": [
    "#### config['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927688da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
